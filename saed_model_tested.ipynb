{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "saed-model-tested.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "y3soCg0XehdF",
        "outputId": "0278437e-4ffe-411c-9410-2e4c77219c92"
      },
      "source": [
        "#Labels are binary 1 is real and 0 is Fake \r\n",
        "\r\n",
        "#practicing regilar expressions \r\n",
        "import re\r\n",
        "import pandas as pd\r\n",
        "import nltk\r\n",
        "from nltk.tokenize import sent_tokenize as st, word_tokenize as wt\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from wordcloud import WordCloud\r\n",
        "import string\r\n",
        "from nltk.corpus import stopwords\r\n",
        "import numpy as np\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "#---------------------Using cropped data-------------------------------\r\n",
        "basiccsv=pd.read_csv('train.csv')\r\n",
        "data_split=np.random.rand(len(basiccsv))<0.50# take true/false table to split the data, all true will go traiting , all false will fo testing data\r\n",
        "df_with_links=basiccsv[~data_split]\r\n",
        "print(df_with_links)\r\n",
        "#---------------------Using cropped data-------------------------------\r\n",
        "\r\n",
        "#df_testing=pd.read_csv('capstone_file.csv')\r\n",
        "print('first')\r\n",
        "print(df_with_links['label'].value_counts())\r\n",
        "\r\n",
        "#PREPROCESSING \r\n",
        "nltk.download('stopwords')\r\n",
        "stop_words=set(stopwords.words('english'))\r\n",
        "# method to remove URL's, @ and # from the text\r\n",
        "def remove_urls_at_hash(new_list,data_column):\r\n",
        "    for t in data_column:\r\n",
        "        newT=re.sub('http://\\S+|https://\\S+','',t) #removing http/https\r\n",
        "        no_at=re.sub('@\\S+|#\\S+|\\n|RT|rt','',newT) # removing @,#,\\n,rt\r\n",
        "        if newT!=no_at : \r\n",
        "            newT=no_at\r\n",
        "        if t==newT:\r\n",
        "            new_list.append(t)\r\n",
        "        elif t!=newT:\r\n",
        "            new_list.append(newT)\r\n",
        "        \r\n",
        " \r\n",
        "#method to remove 1)punctuation 2)stop_words           \r\n",
        "def text_cleaning(txt):\r\n",
        "    cleaned_tweets=[] #list\r\n",
        "    def hasNumbers(inputString):\r\n",
        "      return any(char.isdigit() for char in inputString)\r\n",
        "    no_dig_tweets=[]  \r\n",
        "    temp_no_dig_tweets=[] \r\n",
        "    for tw_1 in txt:\r\n",
        "      for this_word in tw_1.split():\r\n",
        "        if not hasNumbers(this_word):\r\n",
        "          temp_no_dig_tweets.append(this_word.lower())\r\n",
        "        word=\" \".join(temp_no_dig_tweets)\r\n",
        "      no_dig_tweets.append(word)\r\n",
        "      temp_no_dig_tweets=[]\r\n",
        "\r\n",
        "    for tweet in no_dig_tweets:\r\n",
        "        text_remove_punt=[char for char in tweet if char not in string.punctuation] #in char is not in punctuation list then return\r\n",
        "        text_gather_chars_into_words=''.join(text_remove_punt).split()#make 1 long string and split into words\r\n",
        "        text_remove_stop_words=[w for w in text_gather_chars_into_words if w.lower() not in stop_words] #if word not in list then  return \r\n",
        "        cleaned_tweets.append(text_remove_stop_words)\r\n",
        "    return cleaned_tweets\r\n",
        "\r\n",
        "# method to remove duplicate tweets \r\n",
        "def remove_duplicates(df_for_removing_duplicates):\r\n",
        "    no_dub=pd.DataFrame.drop_duplicates(df_for_removing_duplicates)\r\n",
        "    return no_dub\r\n",
        "\r\n",
        "#METHOD CALLS\r\n",
        "no_duplicates=remove_duplicates(df_with_links)\r\n",
        "print('no dublicates first one ')\r\n",
        "print(no_duplicates['label'].value_counts())\r\n",
        "\r\n",
        "rem=[]   #list\r\n",
        "remove_urls_at_hash(new_list=rem,data_column=no_duplicates['text'])\r\n",
        "clean_data=text_cleaning(rem) # removing punctuation, stop words\r\n",
        "\r\n",
        "#to make a from a nested list regular list\r\n",
        "clean_data_list=[]\r\n",
        "for ii in clean_data:\r\n",
        "    clean_data_list.append(' '.join(ii))\r\n",
        "print('len of the clean data after all pre processing is :')\r\n",
        "print(len(clean_data_list))\r\n",
        "\r\n",
        "final_clean_df=pd.DataFrame(clean_data_list) # casting into Data Frame, so we can use it for vectorization\r\n",
        "final_clean_set=list(final_clean_df[0])\r\n",
        "\r\n",
        "#Beg of Words using CountVectorizer from sklearn\r\n",
        "\r\n",
        "\r\n",
        "vectorizer=CountVectorizer()\r\n",
        "x=vectorizer.fit_transform(final_clean_set)#get vocab then build vector then make an array\r\n",
        "features_extracted=vectorizer.vocabulary_\r\n",
        "\r\n",
        "\r\n",
        "# i need list [] --> pu labels in the list \r\n",
        "future_label=[]\r\n",
        "for r in no_duplicates['label']:\r\n",
        "  future_label.append(r)\r\n",
        "print(future_label)\r\n",
        "# NEXT STEP IS TO ADD LABELING TO OUR DATASET\r\n",
        "# 1.1 after vectorization we have as a result a matrix\r\n",
        "matrix_to_array=x.toarray() #1.2 make matrix to array\r\n",
        "array_to_df=pd.DataFrame(matrix_to_array)# an array to dataframe\r\n",
        "array_to_df['label']=future_label # now append label to the df\r\n",
        "print(array_to_df.shape)\r\n",
        "print(array_to_df['label'].value_counts())\r\n",
        "#print(df_no_duplicate)\r\n",
        "#print(rem)\r\n",
        "#use collection import counter to see what words are mostly used to identofi stopwords\r\n",
        "\r\n",
        "print('finally  ')\r\n",
        "print(array_to_df['label'].value_counts())\r\n",
        "#--------------------------------vectorizing clean testing data----------------------------------\r\n",
        "\r\n",
        "#\"\"\"Splitting the data\"\"\"\r\n",
        "\r\n",
        "# DEVIDE INTO TRAINING AND TESTING SET \r\n",
        "data_split=np.random.rand(len(array_to_df))<0.80# take true/false table to split the data, all true will go traiting , all false will fo testing data\r\n",
        "train=array_to_df[data_split] # training set based on randomly selested values 80%\r\n",
        "test=array_to_df[~data_split] # testing set based on 20% randomly seleted values\r\n",
        "print(' Size of training set : ', len(train))\r\n",
        "print(' Size of testing set : ', len(test))\r\n",
        "#print(features_extracted) # gives the vocablulary list\r\n",
        "print('training data shape is')\r\n",
        "print(x.shape) # representation of (tweet number, voablulary list)\r\n",
        "\r\n",
        "# split into independednt and dependednt value for training\r\n",
        "leng=len(train.axes[1])-1\r\n",
        "x_train=train.iloc[:,0:leng]\r\n",
        "y_train=train.iloc[:,leng]\r\n",
        "\r\n",
        "# split into independednt and dependednt value for testing\r\n",
        "x_test=test.iloc[:,0:leng]\r\n",
        "y_test=test.iloc[:,leng]\r\n",
        "\r\n",
        "print(array_to_df['label'].value_counts())\r\n",
        "\r\n",
        "\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.metrics import confusion_matrix\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import seaborn as sns\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "from sklearn.linear_model import LogisticRegression\r\n",
        "LR = LogisticRegression(C=0.5, solver='saga',multi_class='multinomial',class_weight='balanced',max_iter=200).fit(x_train,y_train)\r\n",
        "yhat=LR.predict(x_test)\r\n",
        "print('metrics.accuracy_score : ',metrics.accuracy_score(y_test,yhat ))\r\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "          id  ... label\n",
            "2          2  ...     1\n",
            "3          3  ...     1\n",
            "5          5  ...     0\n",
            "6          6  ...     1\n",
            "7          7  ...     0\n",
            "...      ...  ...   ...\n",
            "20794  20794  ...     0\n",
            "20795  20795  ...     0\n",
            "20796  20796  ...     0\n",
            "20798  20798  ...     1\n",
            "20799  20799  ...     1\n",
            "\n",
            "[10436 rows x 5 columns]\n",
            "first\n",
            "1    5287\n",
            "0    5149\n",
            "Name: label, dtype: int64\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "no dublicates first one \n",
            "1    5287\n",
            "0    5149\n",
            "Name: label, dtype: int64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-9998f5926aeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mrem\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m   \u001b[0;31m#list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m \u001b[0mremove_urls_at_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_column\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mno_duplicates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0mclean_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# removing punctuation, stop words\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-9998f5926aeb>\u001b[0m in \u001b[0;36mremove_urls_at_hash\u001b[0;34m(new_list, data_column)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mremove_urls_at_hash\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_column\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mnewT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'http://\\S+|https://\\S+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#removing http/https\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0mno_at\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'@\\S+|#\\S+|\\n|RT|rt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnewT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# removing @,#,\\n,rt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewT\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0mno_at\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 194\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3KFEtwdo52F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJbnJ1VjfD0O",
        "outputId": "f3c17e0e-8588-4a5a-9c58-96e9bb7d83f2"
      },
      "source": [
        "# saving the model by using joblib\r\n",
        "from sklearn.externals import joblib\r\n",
        "\r\n",
        "joblib.dump(LR, '/content/lr.pkl')\r\n",
        "model = joblib.load(open(\"/content/lr.pkl\", 'rb')) #loading the model\r\n",
        "read_test=pd.read_csv('test.csv') # loading a new testing data from the file \r\n",
        "col=read_test['text']\r\n",
        "\r\n",
        "empt_list_test=[] \r\n",
        "for l in col: #loop over column to store it into a list as vectorizer takes list as a paramiter\r\n",
        "  empt_list_test.append(l)\r\n",
        "\r\n",
        "vectorized_sentence_1=vectorizer.transform(empt_list_test[50:100]) #vectorization, making a matrix using old vocab\r\n",
        "model.predict(vectorized_sentence_1)#predicting the news as fake/real\r\n",
        "\r\n"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['FAKE', 'REAL', 'REAL', 'FAKE', 'REAL', 'REAL', 'FAKE', 'FAKE',\n",
              "       'REAL', 'REAL', 'FAKE', 'REAL', 'FAKE', 'FAKE', 'FAKE', 'FAKE',\n",
              "       'FAKE', 'FAKE', 'REAL', 'REAL', 'REAL', 'FAKE', 'FAKE', 'FAKE',\n",
              "       'FAKE', 'FAKE', 'REAL', 'REAL', 'FAKE', 'FAKE', 'REAL', 'FAKE',\n",
              "       'FAKE', 'FAKE', 'FAKE', 'FAKE', 'FAKE', 'REAL', 'FAKE', 'FAKE',\n",
              "       'FAKE', 'REAL', 'REAL', 'FAKE', 'FAKE', 'REAL', 'REAL', 'REAL',\n",
              "       'FAKE', 'FAKE'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    }
  ]
}