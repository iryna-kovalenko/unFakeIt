# -*- coding: utf-8 -*-
"""Capstone_using_labed_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-GiZThroGMmLtcsJDLIy30w1GkI4m9Bk

Preprocessing
"""

#Labels are binary 1 is real and 0 is Fake 

#practicing regilar expressions 
import re
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize as st, word_tokenize as wt
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import string
from nltk.corpus import stopwords
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

#---------------------Using cropped data-------------------------------
basiccsv=pd.read_csv('fake_or_real_news.csv')
data_split=np.random.rand(len(basiccsv))<0.50# take true/false table to split the data, all true will go traiting , all false will fo testing data
df_with_links=basiccsv[~data_split]
print(df_with_links)
#---------------------Using cropped data-------------------------------

#df_testing=pd.read_csv('capstone_file.csv')
print('first')
print(df_with_links['label'].value_counts())

#PREPROCESSING 
nltk.download('stopwords')
stop_words=set(stopwords.words('english'))
# method to remove URL's, @ and # from the text
def remove_urls_at_hash(new_list,data_column):
    for t in data_column:
        newT=re.sub('http://\S+|https://\S+','',t) #removing http/https
        no_at=re.sub('@\S+|#\S+|\n|RT|rt','',newT) # removing @,#,\n,rt
        if newT!=no_at : 
            newT=no_at
        if t==newT:
            new_list.append(t)
        elif t!=newT:
            new_list.append(newT)
        
 
#method to remove 1)punctuation 2)stop_words           
def text_cleaning(txt):
    cleaned_tweets=[] #list
    def hasNumbers(inputString):
      return any(char.isdigit() for char in inputString)
    no_dig_tweets=[]  
    temp_no_dig_tweets=[] 
    for tw_1 in txt:
      for this_word in tw_1.split():
        if not hasNumbers(this_word):
          temp_no_dig_tweets.append(this_word.lower())
        word=" ".join(temp_no_dig_tweets)
      no_dig_tweets.append(word)
      temp_no_dig_tweets=[]

   # lets see ....
# method to remove duplicate tweets 
def remove_duplicates(df_for_removing_duplicates):
    no_dub=pd.DataFrame.drop_duplicates(df_for_removing_duplicates)
    return no_dub

#METHOD CALLS
no_duplicates=remove_duplicates(df_with_links)
print('no dublicates first one ')
print(no_duplicates['label'].value_counts())

rem=[]   #list
remove_urls_at_hash(new_list=rem,data_column=no_duplicates['text'])
clean_data=text_cleaning(rem) # removing punctuation, stop words

#to make a from a nested list regular list
clean_data_list=[]
for ii in clean_data:
    clean_data_list.append(' '.join(ii))
print('len of the clean data after all pre processing is :')
print(len(clean_data_list))

final_clean_df=pd.DataFrame(clean_data_list) # casting into Data Frame, so we can use it for vectorization
final_clean_set=list(final_clean_df[0])

#TESTING --> comparison between original tweet and tweet after preprocessing 
#print(final_clean_df[5:10])
#print('-----------------')
#print(no_duplicates['text'][5:10])

#Beg of Words using CountVectorizer from sklearn


vectorizer=CountVectorizer()
x=vectorizer.fit_transform(final_clean_set)#get vocab then build vector then make an array
features_extracted=vectorizer.vocabulary_


# i need list [] --> pu labels in the list 
future_label=[]
for r in no_duplicates['label']:
  future_label.append(r)
print(future_label)
# NEXT STEP IS TO ADD LABELING TO OUR DATASET
# 1.1 after vectorization we have as a result a matrix
matrix_to_array=x.toarray() #1.2 make matrix to array
array_to_df=pd.DataFrame(matrix_to_array)# an array to dataframe
array_to_df['label']=future_label # now append label to the df
print(array_to_df.shape)
print(array_to_df['label'].value_counts())
#print(df_no_duplicate)
#print(rem)
#use collection import counter to see what words are mostly used to identofi stopwords

print('finally  ')
print(array_to_df['label'].value_counts())
#--------------------------------vectorizing clean testing data----------------------------------

"""Splitting the data"""

# DEVIDE INTO TRAINING AND TESTING SET 
data_split=np.random.rand(len(array_to_df))<0.80# take true/false table to split the data, all true will go traiting , all false will fo testing data
train=array_to_df[data_split] # training set based on randomly selested values 80%
test=array_to_df[~data_split] # testing set based on 20% randomly seleted values
print(' Size of training set : ', len(train))
print(' Size of testing set : ', len(test))
#print(features_extracted) # gives the vocablulary list
print('training data shape is')
print(x.shape) # representation of (tweet number, voablulary list)

# split into independednt and dependednt value for training
leng=len(train.axes[1])-1
x_train=train.iloc[:,0:leng]
y_train=train.iloc[:,leng]

# split into independednt and dependednt value for testing
x_test=test.iloc[:,0:leng]
y_test=test.iloc[:,leng]

print(array_to_df['label'].value_counts())

"""**K nearest neighbor** (finding the best K)"""

#KNeighborsClassifier for binary data
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
#original testing data
#k=1 -->52%
#k=2 -->47% 
#k=3 -->54%
#k=4 --> 54%
#k=5 --> 54%
#new testing data KNeighborsClassifier
#k=1,0.6353779429987608
#k=2,0.45322180916976457
#k=3,0.7022924411400248
#k=4,0.5111524163568774
#k=5,0.7351301115241635
#k=6,0.6071871127633209
#k=7,0.8116480793060719--> wow xD
#k=8,0.7214993804213135
Ks = 10
for n in range(1,Ks):
  model_one = KNeighborsClassifier(n_neighbors = n ,metric='matching',).fit(x_train,y_train)
  yhat=model_one.predict(x_test) 
  print(" using matching metric is ",metrics.accuracy_score(y_test, yhat)) 

  cm = pd.crosstab(yhat,y_test,rownames=['Predicted'], colnames=['Actual'])
  plt.figure(figsize = (10, 10))
  sns.heatmap(cm, annot = True,fmt='d')

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(C=0.5, solver='saga',multi_class='multinomial',class_weight='balanced',max_iter=200).fit(x_train,y_train)
yhat=LR.predict(x_test)
print('metrics.accuracy_score : ',metrics.accuracy_score(y_test,yhat ))

from sklearn.linear_model import LogisticRegression
LR = LogisticRegression(solver='saga',multi_class='multinomial',max_iter=200).fit(x_train,y_train)
yhat=LR.predict(x_test)
print('metrics.accuracy_score : ',metrics.accuracy_score(y_test,yhat ))

print(df_no_duplicate)

from sklearn.naive_bayes import ComplementNB
model_ComplementNB = ComplementNB(norm="True").fit(x_train, y_train)
yhat = model_ComplementNB.predict(x_test)
print('ComplementNB accuracy_score : ',metrics.accuracy_score(y_test, yhat))

cm = pd.crosstab(yhat,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

from sklearn.naive_bayes import GaussianNB

model_GaussianNB = GaussianNB().fit(x_train, y_train)
yhat = model_GaussianNB.predict(x_test)
print('GaussianNB accuracy_score : ',metrics.accuracy_score(y_test, yhat))

"""testing shit"""

basiccsv=pd.read_csv('fake_or_real_news.csv')
data_split=np.random.rand(len(basiccsv))<0.70# take true/false table to split the data, all true will go traiting , all false will fo testing data
df_with_links=basiccsv[~data_split]
no_dub=pd.DataFrame.drop_duplicates(df_with_links)
print(no_dub['label'].value_counts())

#print(final_clean_df[0])
final_clean_set=set(final_clean_df[0])
print('----------------------')
print(final_clean_set)

#print(array_to_df['label'])
print('----------------------')
print('----------------------')
print('----------------------')
print('----------------------')
print('----------------------')
print('----------------------')
print(no_duplicates['label'])

#print(no_duplicates['text'])
print(len((no_duplicates['label']).to_string(index=False)))
print(len((no_duplicates['text']).to_string(index=False)))
'''
new_df_fuck=[]
array_to_df
new_df_fuck['text']=(no_duplicates['text'].to_string(index=False))
print(new_df_fuck)

array_to_df=pd.DataFrame(matrix_to_array)# an array to dataframe
array_to_df['label']=no_duplicates['label']
'''
# i need list []
future_label=[]
for r in no_duplicates['label']:
  future_label.append(r)
print(len(no_duplicates['label']))
print(len(future_label))

print(len(no_duplicates['text']))
print(len(no_duplicates['label']))

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from collections import Counter
from nltk.tokenize import word_tokenize
from sklearn.metrics import classification_report

model1=PassiveAggressiveClassifier(max_iter=300)
model1.fit(x_train,y_train)
y_pred1=model1.predict(x_test)
acc1=accuracy_score(y_test,y_pred1)
print('PassiveAggressiveClassifier',acc1)

cm = pd.crosstab(y_pred1,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model3=DecisionTreeClassifier()
model3.fit(x_train,y_train)
y_pred3=model3.predict(x_test)
acc3=accuracy_score(y_test,y_pred3)
print('DecisionTreeClassifier',acc3)

cm = pd.crosstab(y_pred3,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model4=RandomForestClassifier()
model4.fit(x_train,y_train)
y_pred4=model4.predict(x_test)
acc4=accuracy_score(y_test,y_pred4)
print('RandomForestClassifier',acc4)

cm = pd.crosstab(y_pred4,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model5=SVC()
model5.fit(x_train,y_train)
y_pred5=model5.predict(x_test)
acc5=accuracy_score(y_test,y_pred5)
print('RandomForestClassifier',acc5)

cm = pd.crosstab(y_pred5,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')

model6=LogisticRegression()
model6.fit(x_train,y_train)
y_pred6=model6.predict(x_test)
acc6=accuracy_score(y_test,y_pred6)
print('LogisticRegression',acc6)

cm = pd.crosstab(y_pred6,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')
print(classification_report(y_test,y_pred6))

#Labels are binary 1 is real and 0 is Fake 

#practicing regilar expressions 
import re
import pandas as pd
import nltk
from nltk.tokenize import sent_tokenize as st, word_tokenize as wt
from sklearn.feature_extraction.text import CountVectorizer
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import string
from nltk.corpus import stopwords
import numpy as np
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

#---------------------Using cropped data-------------------------------
basiccsv=pd.read_csv('mini_news.csv')
data_split=np.random.rand(len(basiccsv))<0.70# take true/false table to split the data, all true will go traiting , all false will fo testing data
df_with_links=basiccsv[data_split]
print(df_with_links)
#---------------------Using cropped data-------------------------------
#PREPROCESSING 
nltk.download('stopwords')
stop_words=set(stopwords.words('english'))
# method to remove URL's, @ and # from the text
def remove_urls_at_hash(new_list,data_column):
    for t in data_column:
        newT=re.sub('http://\S+|https://\S+','',t) #removing http/https
        no_at=re.sub('@\S+|#\S+|\n|RT|rt','',newT) # removing @,#,\n,rt
        if newT!=no_at : 
            newT=no_at
        if t==newT:
            new_list.append(t)
        elif t!=newT:
            new_list.append(newT)
        
 
#method to remove 1)punctuation 2)stop_words           
def text_cleaning(txt):
    cleaned_tweets=[] #list
    def hasNumbers(inputString):
      return any(char.isdigit() for char in inputString)
    no_dig_tweets=[]  
    temp_no_dig_tweets=[] 
    for tw_1 in txt:
      for this_word in tw_1.split():
        if not hasNumbers(this_word):
          temp_no_dig_tweets.append(this_word.lower())
        word=" ".join(temp_no_dig_tweets)
      no_dig_tweets.append(word)
      temp_no_dig_tweets=[]

    for tweet in no_dig_tweets:
        text_remove_punt=[char for char in tweet if char not in string.punctuation] #in char is not in punctuation list then return
        text_gather_chars_into_words=''.join(text_remove_punt).split()#make 1 long string and split into words
        text_remove_stop_words=[w for w in text_gather_chars_into_words if w.lower() not in stop_words] #if word not in list then  return 
        cleaned_tweets.append(text_remove_stop_words)
    return cleaned_tweets

# method to remove duplicate tweets 
def remove_duplicates(df_for_removing_duplicates):
    no_dub=pd.DataFrame.drop_duplicates(df_for_removing_duplicates)
    return no_dub

#METHOD CALLS
no_duplicates=remove_duplicates(df_with_links)
rem=[]   #list
remove_urls_at_hash(new_list=rem,data_column=no_duplicates['text'])
clean_data=text_cleaning(rem) # removing punctuation, stop words

#to make a from a nested list regular list
clean_data_list=[]
for ii in clean_data:
    clean_data_list.append(' '.join(ii))
print('len of the clean data after all pre processing is :')
print(len(clean_data_list))

final_clean_df=pd.DataFrame(clean_data_list) # casting into Data Frame, so we can use it for vectorization
final_clean_set=list(final_clean_df[0])

print('pretend you didnt see this below ')

vectorizer=CountVectorizer()
x=vectorizer.transform(final_clean_set)#get vocab then build vector then make an array
print('training data shape is')
print(x.shape) # representation of (tweet number, voablulary list)

# i need list [] --> pu labels in the list 
future_label=[]
for r in no_duplicates['label']:
  future_label.append(r)
print(future_label)
# NEXT STEP IS TO ADD LABELING TO OUR DATASET
# 1.1 after vectorization we have as a result a matrix
matrix_to_array=x.toarray() #1.2 make matrix to array
array_to_df=pd.DataFrame(matrix_to_array)# an array to dataframe
array_to_df['label']=future_label # now append label to the df
print(array_to_df.shape)
print(array_to_df['label'].value_counts())
#print(df_no_duplicate)
#print(rem)
#use collection import counter to see what words are mostly used to identofi stopwords

print('finally  ')
print(array_to_df['label'].value_counts())
#--------------------------------vectorizing clean testing data----------------------------------

from sklearn.linear_model import PassiveAggressiveClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd
from sklearn.naive_bayes import GaussianNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
from mlxtend.plotting import plot_confusion_matrix
from collections import Counter
from nltk.tokenize import word_tokenize
from sklearn.metrics import classification_report

model6=LogisticRegression()
model6.fit(x_train,y_train)
y_pred6=model6.predict(x_test)
acc6=accuracy_score(y_test,y_pred6)
print('LogisticRegression',acc6)

cm = pd.crosstab(y_pred6,y_test,rownames=['Predicted'], colnames=['Actual'])
plt.figure(figsize = (10, 10))
sns.heatmap(cm, annot = True,fmt='d')
print(classification_report(y_test,y_pred6))

